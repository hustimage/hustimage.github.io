{
  "name": "图像分析与理解",
  "tagline": "华中科技大学《图像分析与理解》课程项目进度汇报",
  "body": "# 项目名称：3D Rigid Object Tracking\r\n## 小组成员：巨荣辉、谢存煌、柯旺松、张兴龙\r\n\r\n###2016年5月19日更新    \r\n\r\n选择项目3D Rigid Object Tracking，项目开始。\r\n\r\n###2016年5月26日更新：\r\n* . 阅读项目中所给文献，了解本项目所需关键技术；\r\n\r\n* . 组内成员对matlab较为熟悉，暂定其作为编程平台完成相关内容，后期如有需要再移植至其他平台；\r\n\r\n* . 因为3D Rigid Object Tracking 需要调动摄像头进行相关图像处理内容，完成简单的matlab摄像头调用代码。\r\n\r\n    `clear all; clc`\r\n\r\n    ` vid = videoinput('winvideo', 1, 'YUY2_640x480');%创建ID为1的摄像头的视频对象`\r\n\r\n    ` set(vid,'ReturnedColorSpace','rgb');`\r\n\r\n    ` vidRes=get(vid,'VideoResolution');`\r\n\r\n    ` width=vidRes(1);`\r\n\r\n    ` height=vidRes(2);`\r\n\r\n    ` nBands=get(vid,'NumberOfBands');`\r\n\r\n    ` figure('Name', 'Matlab调用摄像头 ', 'NumberTitle', 'Off', 'ToolBar', 'None', 'MenuBar', 'None');`\r\n\r\n    ` hImage=image(zeros(vidRes(2),vidRes(1),nBands));`\r\n\r\n    ` preview(vid,hImage);    %打开视频预览窗口`\r\n\r\n![matlab调用摄像头](http://i4.buimg.com/eca3297b1e2722a3.png)\r\n（不知道如何插入视频=_=）\r\n\r\n* . matlab调用摄像头成功之后，通过对每一帧进行解析，进行简单的图像处理操作，通过帧差的方法实现简单的物体跟踪，当然不是很稳定，后期进行改  进。通过帧差方法可以得到大致的运动物体所在区域，为后期工作中特征匹配的实现奠定一个基础，如图是实现的效果（视频不知道怎么插入=_=）。\r\n\r\n![简单物体识别](http://i2.buimg.com/8f51be16b9025a4a.png)\r\n   \r\n* . 后续需要进行的工作包括物体特征的提取，视频实时的物体匹配识别，AR实现等。\r\n（本次更新到此为止）\r\n\r\n###2016年6月2日更新：\r\n* . 为了更好地了解项目的相关知识，我们选择了2篇文献去学习；\r\n\r\n* . 我们主要阅读了《Keyframe-based Modeling and Tracking of Multiple 3D Objects》这篇文献：\r\n\r\n本文主要提出了一种实现了检测和帧到帧的追踪的对多个3D物体进行追踪的算法。该方法的主要缺点是，随着图像内超过10个物体时，它在计算时间和数据存储方面大大增加，从而变得不切实际。\r\n\r\n对于一个给定的输入帧，检测模块返回由相似性得分排序的关键帧的列表。我们认为每个物体至少有一个关键帧数据库。如果一个物体比另一个展示出更多的得分点，它往往会人为地得到更好的得分，并且在最好的关键帧更频繁地出现。然后，通过采用RANSAC计算目标pose，我们尝试强每个关键帧与输入图像匹配。\r\n\r\n![论文中出现的结果1](http://7xrn7f.com1.z0.glb.clouddn.com/16-6-2/1895005.jpg)\r\n\r\n![论文中出现的结果2](http://7xrn7f.com1.z0.glb.clouddn.com/16-6-2/84021982.jpg)\r\n\r\n（a）系统连续地重建环境和从视频中跟踪相机流；\r\n（b）在添加新的物体时，用户通过使用刷状工具选择物体的某些功能；\r\n（c）系统可以调整展示的平面信息\r\n（d）扩展到三维框\r\n（e）（f）物体可以在场景内随意的移动\r\n（g-k）已知的物体当它们被看到时就可被识别，并且被独立的追踪\r\n\r\n* . 接下来的一周需要我们通过文献中学到的算法，从而来实现整个项目\r\n（本次更新到此为止）\r\n\r\n###2016年6月9日更新：\r\n\r\n* . 通过之前两周的文献阅读和先期工作，我们确定了项目所使用的主要方法：\r\n\r\n使用Camshift算法来实现对物体的追踪，并使用卡尔曼滤波来预测物体接下来的位置以防止出现遮挡的情况。\r\n\r\n* . camshift算法\r\n\r\ncamshift利用目标的颜色直方图模型将图像转换为颜色概率分布图，初始化一个搜索窗的大小和位置，并根据上一帧得到的结果自适应调整搜索窗口的位置和大小，从而定位出当前图像中目标的中心位置。\r\n\r\n色彩投影图（反向投影）：(1).RGB颜色空间对光照亮度变化较为敏感，为了减少此变化对跟踪效果的影响，首先将图像从RGB空间转换到HSV空间。(2).然后对其中的H分量作直方图，在直方图中代表了不同H分量值出现的概率或者像素个数，就是说可以查找出H分量大小为h的概率或者像素个数，即得到了颜色概率查找表。(3).将图像中每个像素的值用其颜色出现的概率对替换，就得到了颜色概率分布图。这个过程就叫反向投影，颜色概率分布图是一个灰度图像。\r\n\r\ncamshift：将meanshift算法扩展到连续图像序列，就是camshift算法。它将视频的所有帧做meanshift运算，并将上一帧的结果，即搜索窗的大小和中心，作为下一帧meanshift算法搜索窗的初始值。如此迭代下去，就可以实现对目标的跟踪。\r\n\r\n算法过程为：\r\n(1).初始化搜索窗\r\n(2).计算搜索窗的颜色概率分布（反向投影）\r\n(3).运行meanshift算法，获得搜索窗新的大小和位置。\r\n(4).在下一帧视频图像中用(3)中的值重新初始化搜索窗的大小和位置，再跳转到(2)继续进行。\r\n\r\n* . 对于物体的追踪，如果背景单一，即你要跟踪的物体颜色和背景色有较大区别，可用基于颜色的跟踪如camshift，鲁棒性都是较好的；如果背景复杂,如背景中有和前景一样的颜色，就需要用到一些具有预测性的算法，如卡尔曼滤波等，可以和CAMSHIFT结合。\r\n\r\n* . 所以接下来的一周我们将会根据以上算法来实现项目的内容。\r\n（本次更新到此为止）\r\n\r\n###2016年6月16日更新：\r\n\r\n* . 本周是项目的最后一周，我们也会在这次更新中展示出项目的demo，下面会给出相应的演示视频。\r\n\r\n![项目效果图](http://7xrn7f.com1.z0.glb.clouddn.com/16-6-18/6949119.jpg)\r\n\r\n效果图中，我们用蓝色边框将物体框出，并不断地对其进行追踪，而图中的绿色小×代表了用camshift算法得到的物体的质心，粉红色的小×代表了卡尔曼滤波得到的预测的质心。具体的更多细节可参见本次更新最后的demo视频。\r\n\r\n环境配置：OpenCv3.1   Visual Studio 2013   Windows 10 64位系统\r\n\r\n目标跟踪速度： >30 FPS\r\n\r\n图像分析与理解课程项目演示视频：\r\n<iframe height=498 width=510 src=\"http://player.youku.com/embed/XMTYxMjcxNzE2OA==\" frameborder=0 allowfullscreen></iframe>",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}